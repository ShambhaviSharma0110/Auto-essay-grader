{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShambhaviSharma0110/Auto-essay-grader/blob/main/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcStkO7EdvUJ"
      },
      "source": [
        "### FEATURE EXTRACTION "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XK4n2z7qdvUM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import re, collections\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FKGE0avWdvUN"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('set5.csv', encoding = 'ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBDzOPmvdvUO",
        "outputId": "a6a9139d-892d-4948-e82d-e25479f7f333"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11827</td>\n",
              "      <td>In this memoir of Narciso Rodriguez, @PERSON3'...</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11828</td>\n",
              "      <td>Throughout the excerpt from Home the Blueprint...</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11829</td>\n",
              "      <td>The mood the author created in the memoir is l...</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11830</td>\n",
              "      <td>The mood created by the author is showing how ...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11831</td>\n",
              "      <td>The mood created in the memoir is happiness an...</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>13627</td>\n",
              "      <td>The mood of this memoir is nonfiction. The moo...</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>13628</td>\n",
              "      <td>The mood was created by the author in the memo...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>13629</td>\n",
              "      <td>In the memoir \"Narciso Rodriguez\", the mood cr...</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>13630</td>\n",
              "      <td>The mood created @CAPS3 the author, Narciso Ro...</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>13631</td>\n",
              "      <td>The author created such a specific mood for th...</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1805 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      essay_id                                              essay  \\\n",
              "0        11827  In this memoir of Narciso Rodriguez, @PERSON3'...   \n",
              "1        11828  Throughout the excerpt from Home the Blueprint...   \n",
              "2        11829  The mood the author created in the memoir is l...   \n",
              "3        11830  The mood created by the author is showing how ...   \n",
              "4        11831  The mood created in the memoir is happiness an...   \n",
              "...        ...                                                ...   \n",
              "1800     13627  The mood of this memoir is nonfiction. The moo...   \n",
              "1801     13628  The mood was created by the author in the memo...   \n",
              "1802     13629  In the memoir \"Narciso Rodriguez\", the mood cr...   \n",
              "1803     13630  The mood created @CAPS3 the author, Narciso Ro...   \n",
              "1804     13631  The author created such a specific mood for th...   \n",
              "\n",
              "      domain1_score  Unnamed: 3  Unnamed: 4  \n",
              "0                 2         NaN         NaN  \n",
              "1                 2         NaN         NaN  \n",
              "2                 3         NaN         NaN  \n",
              "3                 1         NaN         NaN  \n",
              "4                 3         NaN         NaN  \n",
              "...             ...         ...         ...  \n",
              "1800              2         NaN         NaN  \n",
              "1801              0         NaN         NaN  \n",
              "1802              4         NaN         NaN  \n",
              "1803              3         NaN         NaN  \n",
              "1804              2         NaN         NaN  \n",
              "\n",
              "[1805 rows x 5 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vk9RRtkKdvUO"
      },
      "outputs": [],
      "source": [
        "data.drop(data.iloc[:, 3:5], inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4rpKoUnPdvUP",
        "outputId": "5e5204dc-8bf5-478d-a22b-5249f755f22a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      essay_id                                              essay  \\\n",
              "0        11827  In this memoir of Narciso Rodriguez, @PERSON3'...   \n",
              "1        11828  Throughout the excerpt from Home the Blueprint...   \n",
              "2        11829  The mood the author created in the memoir is l...   \n",
              "3        11830  The mood created by the author is showing how ...   \n",
              "4        11831  The mood created in the memoir is happiness an...   \n",
              "...        ...                                                ...   \n",
              "1800     13627  The mood of this memoir is nonfiction. The moo...   \n",
              "1801     13628  The mood was created by the author in the memo...   \n",
              "1802     13629  In the memoir \"Narciso Rodriguez\", the mood cr...   \n",
              "1803     13630  The mood created @CAPS3 the author, Narciso Ro...   \n",
              "1804     13631  The author created such a specific mood for th...   \n",
              "\n",
              "      domain1_score  \n",
              "0                 2  \n",
              "1                 2  \n",
              "2                 3  \n",
              "3                 1  \n",
              "4                 3  \n",
              "...             ...  \n",
              "1800              2  \n",
              "1801              0  \n",
              "1802              4  \n",
              "1803              3  \n",
              "1804              2  \n",
              "\n",
              "[1805 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af979c7c-8460-4da4-8a42-c338eb10ad5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11827</td>\n",
              "      <td>In this memoir of Narciso Rodriguez, @PERSON3'...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11828</td>\n",
              "      <td>Throughout the excerpt from Home the Blueprint...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11829</td>\n",
              "      <td>The mood the author created in the memoir is l...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11830</td>\n",
              "      <td>The mood created by the author is showing how ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11831</td>\n",
              "      <td>The mood created in the memoir is happiness an...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>13627</td>\n",
              "      <td>The mood of this memoir is nonfiction. The moo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>13628</td>\n",
              "      <td>The mood was created by the author in the memo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>13629</td>\n",
              "      <td>In the memoir \"Narciso Rodriguez\", the mood cr...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>13630</td>\n",
              "      <td>The mood created @CAPS3 the author, Narciso Ro...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>13631</td>\n",
              "      <td>The author created such a specific mood for th...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1805 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af979c7c-8460-4da4-8a42-c338eb10ad5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af979c7c-8460-4da4-8a42-c338eb10ad5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af979c7c-8460-4da4-8a42-c338eb10ad5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xOMVQeKKdvUP"
      },
      "outputs": [],
      "source": [
        "def sentence_to_wordlist(raw_sentence):\n",
        "    \n",
        "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
        "    tokens = nltk.word_tokenize(clean_sentence)\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FzHz1qqGdvUP"
      },
      "outputs": [],
      "source": [
        "def tokenize(essay):\n",
        "    stripped_essay = essay.strip()\n",
        "    \n",
        "    raw_sentences = nltk.sent_tokenize(essay)\n",
        "    \n",
        "    tokenized_sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
        "    \n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AH0MS15odvUQ"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 1 (list of refrence paper is at the end of this notebook)\n",
        "def avg_word_len(essay):\n",
        "    \n",
        "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
        "    words = nltk.word_tokenize(clean_essay)\n",
        "    \n",
        "    return sum(len(word) for word in words) / len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PXviTQzWdvUQ"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 1 (list of refrence paper is at the end of this notebook)\n",
        "def word_count(essay):\n",
        "    \n",
        "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
        "    words = nltk.word_tokenize(clean_essay)\n",
        "    \n",
        "    return len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vNxldT1advUQ"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 1 (list of refrence paper is at the end of this notebook)\n",
        "def sent_count(essay):\n",
        "    \n",
        "    sentences = nltk.sent_tokenize(essay)\n",
        "    \n",
        "    return len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tH8c95QydvUQ"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 2 (list of refrence paper is at the end of this notebook)\n",
        "def count_lemmas(essay):\n",
        "    \n",
        "    tokenized_sentences = tokenize(essay)      \n",
        "    \n",
        "    lemmas = []\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    for sentence in tokenized_sentences:\n",
        "        tagged_tokens = nltk.pos_tag(sentence) \n",
        "        \n",
        "        for token_tuple in tagged_tokens:\n",
        "        \n",
        "            pos_tag = token_tuple[1]\n",
        "        \n",
        "            if pos_tag.startswith('N'): \n",
        "                pos = wordnet.NOUN\n",
        "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
        "            elif pos_tag.startswith('J'):\n",
        "                pos = wordnet.ADJ\n",
        "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
        "            elif pos_tag.startswith('V'):\n",
        "                pos = wordnet.VERB\n",
        "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
        "            elif pos_tag.startswith('R'):\n",
        "                pos = wordnet.ADV\n",
        "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
        "            else:\n",
        "                pos = wordnet.NOUN\n",
        "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
        "    \n",
        "    lemma_count = len(set(lemmas))\n",
        "    \n",
        "    return lemma_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install enchant --fix-missing\n",
        "!apt install -qq enchant\n",
        "!pip install pyenchant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnV9LJzvebsm",
        "outputId": "fc080bd5-db86-489f-e1bf-502eb46ce844"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,626 B 0%] [Co\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100%] [Connected to devel\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [3 InRelease 14.2 kB/88.7 kB 16%] [4 InRelease 20.0 kB/88.7 kB 22%] [Waiting\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [3 InRelease 14.2 kB/88.7 kB 16%] [4 InRelease 20.\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [3 InRelease 56.2 kB/88.7 kB 63%] [Waiting for hea\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to ppa.launchpad\u001b[0m\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 242 kB] [5 InRelease 9,842 B/83.3 kB 12%] [Waiting for hea\u001b[0m\u001b[33m\r                                                                               \r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                        \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [6 InRelease 14.2 kB/15.9 kB\u001b[0m\r                                                                               \rIgn:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,235 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,143 kB]\n",
            "Fetched 3,659 kB in 2s (1,505 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,312 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.2 [310 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.2 [87.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,312 kB in 0s (10.9 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "Ji5rcWsWdvUR"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 1 (list of refrence paper is at the end of this notebook)\n",
        "# The feature has been modified although the basic idea comes from this the reference given in above line\n",
        "import enchant\n",
        "def count_spell_error(essay):\n",
        "    \n",
        "    d = enchant.Dict(\"en_US\")\n",
        "    \n",
        "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
        "    words = nltk.word_tokenize(clean_essay)\n",
        "    \n",
        "    mispell = 0\n",
        "    \n",
        "    for word in words:\n",
        "        if(d.check(word)==False):\n",
        "            mispell = mispell + 1\n",
        "    \n",
        "    total_words = word_count(essay)\n",
        "    mispell_prop = mispell/total_words\n",
        "    \n",
        "    return mispell_prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0xiAf820dvUR"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 2 (list of refrence paper is at the end of this notebook)\n",
        "# The feature has been modified\n",
        "def count_pos(essay):\n",
        "    \n",
        "    tokenized_sentences = tokenize(essay)\n",
        "    \n",
        "    noun_count = 0\n",
        "    adj_count = 0\n",
        "    verb_count = 0\n",
        "    adv_count = 0\n",
        "    \n",
        "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
        "    words = nltk.word_tokenize(clean_essay)\n",
        "    \n",
        "    for sentence in tokenized_sentences:\n",
        "        tagged_tokens = nltk.pos_tag(sentence)\n",
        "        \n",
        "        for token_tuple in tagged_tokens:\n",
        "            pos_tag = token_tuple[1]\n",
        "        \n",
        "            if pos_tag.startswith('N'): \n",
        "                noun_count += 1\n",
        "            elif pos_tag.startswith('J'):\n",
        "                adj_count += 1\n",
        "            elif pos_tag.startswith('V'):\n",
        "                verb_count += 1\n",
        "            elif pos_tag.startswith('R'):\n",
        "                adv_count += 1\n",
        "    \n",
        "    return noun_count/len(words), adj_count/len(words), verb_count/len(words), adv_count/len(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT1n93Dzft2C",
        "outputId": "fa8fb437-62d8-46e8-8faa-6121feab2562"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dCcxfFN6dvUR"
      },
      "outputs": [],
      "source": [
        "# The idea for this feature is taken from Reference paper number 4 (list of refrence paper is at the end of this notebook)\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "def sentiment_tagger(essay):\n",
        "    neg_sent = 0\n",
        "    pos_sent = 0\n",
        "    nue_sent = 0\n",
        "    ss = sid.polarity_scores(essay)\n",
        "    for k in sorted(ss):\n",
        "        if k == 'compound':\n",
        "            pass\n",
        "        elif k == 'neg':\n",
        "            neg_sent = neg_sent + ss[k]\n",
        "        elif k == 'pos':\n",
        "            pos_sent = pos_sent + ss[k]\n",
        "        elif k == 'neu':\n",
        "            nue_sent = nue_sent + ss[k]\n",
        "    return neg_sent, pos_sent, nue_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NpNzh7p5dvUS"
      },
      "outputs": [],
      "source": [
        "def get_tfidf_vectors(essays):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    \n",
        "    words = []\n",
        "    for essay in essays:\n",
        "        clean_essay = re.sub(r'\\W', ' ', essay)\n",
        "        words.append(nltk.word_tokenize(clean_essay))\n",
        "        \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    docs_lemmatized = [[lemmatizer.lemmatize(j) for j in i]for i in words]\n",
        "    \n",
        "    corpus = [' '.join(i) for i in docs_lemmatized]\n",
        "    \n",
        "    vectors = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    \n",
        "    return feature_names, vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cDzJpwZf9yU",
        "outputId": "c6ede4ed-5a5a-4bb6-c279-503c1de13ecc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOugZ_mIm-zQ",
        "outputId": "c447a29a-4eae-45f4-92cf-236aa6278eac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ll8-vNmkdvUS"
      },
      "outputs": [],
      "source": [
        "feature_names_cv,vectors_all = get_tfidf_vectors(data['essay'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diQZTxyLdvUS",
        "outputId": "04af15db-1653-4280-bf72-07f6b343738e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1805x4268 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 69696 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "vectors_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM3wdffEdvUS",
        "outputId": "bac3a560-3eb8-4c3f-c247-8a9738589c43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "index_high = data.index[data['domain1_score'] == 4].tolist()\n",
        "n = len(index_high)\n",
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_l9fREbndvUS"
      },
      "outputs": [],
      "source": [
        "# This feature has not been taken from any of the papers\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def get_similarity(essay_id):\n",
        "    j = data.index[data['essay_id'] == essay_id]\n",
        "    similarity = 0\n",
        "    for i in index_high:\n",
        "        similarity += cosine_similarity(vectors_all[i,:],vectors_all[j,:])\n",
        "    similarity /= n\n",
        "    return np.asscalar(similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qifQS6eQdvUS"
      },
      "outputs": [],
      "source": [
        "def extract_features(data):\n",
        "    \n",
        "    features = data.copy()\n",
        "    \n",
        "    features['word_count'] = features['essay'].apply(word_count)\n",
        "    \n",
        "    features['sent_count'] = features['essay'].apply(sent_count)\n",
        "    \n",
        "    features['avg_word_len'] = features['essay'].apply(avg_word_len)\n",
        "    \n",
        "    features['lemma_count'] = features['essay'].apply(count_lemmas)\n",
        "    \n",
        "    features['spell_err_count'] = features['essay'].apply(count_spell_error)\n",
        "    \n",
        "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['essay'].map(count_pos))\n",
        "    \n",
        "    features['neg_score'], features['pos_score'], features['nue_score'] = zip(*features['essay'].map(sentiment_tagger))\n",
        "    \n",
        "    features['similarity'] = features['essay_id'].apply(get_similarity)\n",
        "    \n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Dhb-8c_4dvUS"
      },
      "outputs": [],
      "source": [
        "features_set1 = extract_features(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M2laCmCcdvUT",
        "outputId": "5513fd35-cdd3-4c19-9cb5-7ee89bfba020"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      essay_id                                              essay  \\\n",
              "0        11827  In this memoir of Narciso Rodriguez, @PERSON3'...   \n",
              "1        11828  Throughout the excerpt from Home the Blueprint...   \n",
              "2        11829  The mood the author created in the memoir is l...   \n",
              "3        11830  The mood created by the author is showing how ...   \n",
              "4        11831  The mood created in the memoir is happiness an...   \n",
              "...        ...                                                ...   \n",
              "1800     13627  The mood of this memoir is nonfiction. The moo...   \n",
              "1801     13628  The mood was created by the author in the memo...   \n",
              "1802     13629  In the memoir \"Narciso Rodriguez\", the mood cr...   \n",
              "1803     13630  The mood created @CAPS3 the author, Narciso Ro...   \n",
              "1804     13631  The author created such a specific mood for th...   \n",
              "\n",
              "      domain1_score  word_count  sent_count  avg_word_len  lemma_count  \\\n",
              "0                 2         133           8      4.383459           81   \n",
              "1                 2         168           7      4.285714          102   \n",
              "2                 3         112           6      4.580357           75   \n",
              "3                 1          75           3      4.226667           44   \n",
              "4                 3         127           8      4.283465           68   \n",
              "...             ...         ...         ...           ...          ...   \n",
              "1800              2         132           7      4.393939           71   \n",
              "1801              0          30           1      4.033333           21   \n",
              "1802              4         166           9      4.421687           93   \n",
              "1803              3         132           6      4.492424           86   \n",
              "1804              2          95           6      4.326316           62   \n",
              "\n",
              "      spell_err_count  noun_count  adj_count  verb_count  adv_count  \\\n",
              "0            0.067669    0.255639   0.030075    0.218045   0.022556   \n",
              "1            0.011905    0.220238   0.083333    0.184524   0.071429   \n",
              "2            0.080357    0.321429   0.080357    0.169643   0.035714   \n",
              "3            0.146667    0.240000   0.106667    0.200000   0.013333   \n",
              "4            0.047244    0.228346   0.086614    0.173228   0.070866   \n",
              "...               ...         ...        ...         ...        ...   \n",
              "1800         0.015152    0.318182   0.053030    0.174242   0.037879   \n",
              "1801         0.133333    0.333333   0.000000    0.166667   0.000000   \n",
              "1802         0.066265    0.246988   0.114458    0.222892   0.012048   \n",
              "1803         0.090909    0.318182   0.037879    0.166667   0.060606   \n",
              "1804         0.073684    0.231579   0.042105    0.168421   0.073684   \n",
              "\n",
              "      neg_score  pos_score  nue_score  similarity  \n",
              "0         0.000      0.153      0.847    0.102273  \n",
              "1         0.009      0.189      0.802    0.098348  \n",
              "2         0.000      0.205      0.795    0.169553  \n",
              "3         0.030      0.138      0.832    0.034662  \n",
              "4         0.000      0.260      0.740    0.158431  \n",
              "...         ...        ...        ...         ...  \n",
              "1800      0.000      0.203      0.797    0.106523  \n",
              "1801      0.000      0.148      0.852    0.061781  \n",
              "1802      0.000      0.239      0.761    0.103630  \n",
              "1803      0.000      0.195      0.805    0.165012  \n",
              "1804      0.000      0.221      0.779    0.091583  \n",
              "\n",
              "[1805 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-983accbe-6974-4759-89f9-314a7882e225\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>word_count</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>avg_word_len</th>\n",
              "      <th>lemma_count</th>\n",
              "      <th>spell_err_count</th>\n",
              "      <th>noun_count</th>\n",
              "      <th>adj_count</th>\n",
              "      <th>verb_count</th>\n",
              "      <th>adv_count</th>\n",
              "      <th>neg_score</th>\n",
              "      <th>pos_score</th>\n",
              "      <th>nue_score</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11827</td>\n",
              "      <td>In this memoir of Narciso Rodriguez, @PERSON3'...</td>\n",
              "      <td>2</td>\n",
              "      <td>133</td>\n",
              "      <td>8</td>\n",
              "      <td>4.383459</td>\n",
              "      <td>81</td>\n",
              "      <td>0.067669</td>\n",
              "      <td>0.255639</td>\n",
              "      <td>0.030075</td>\n",
              "      <td>0.218045</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.102273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11828</td>\n",
              "      <td>Throughout the excerpt from Home the Blueprint...</td>\n",
              "      <td>2</td>\n",
              "      <td>168</td>\n",
              "      <td>7</td>\n",
              "      <td>4.285714</td>\n",
              "      <td>102</td>\n",
              "      <td>0.011905</td>\n",
              "      <td>0.220238</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.184524</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.098348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11829</td>\n",
              "      <td>The mood the author created in the memoir is l...</td>\n",
              "      <td>3</td>\n",
              "      <td>112</td>\n",
              "      <td>6</td>\n",
              "      <td>4.580357</td>\n",
              "      <td>75</td>\n",
              "      <td>0.080357</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.080357</td>\n",
              "      <td>0.169643</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.169553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11830</td>\n",
              "      <td>The mood created by the author is showing how ...</td>\n",
              "      <td>1</td>\n",
              "      <td>75</td>\n",
              "      <td>3</td>\n",
              "      <td>4.226667</td>\n",
              "      <td>44</td>\n",
              "      <td>0.146667</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.106667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.832</td>\n",
              "      <td>0.034662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11831</td>\n",
              "      <td>The mood created in the memoir is happiness an...</td>\n",
              "      <td>3</td>\n",
              "      <td>127</td>\n",
              "      <td>8</td>\n",
              "      <td>4.283465</td>\n",
              "      <td>68</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.228346</td>\n",
              "      <td>0.086614</td>\n",
              "      <td>0.173228</td>\n",
              "      <td>0.070866</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.158431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>13627</td>\n",
              "      <td>The mood of this memoir is nonfiction. The moo...</td>\n",
              "      <td>2</td>\n",
              "      <td>132</td>\n",
              "      <td>7</td>\n",
              "      <td>4.393939</td>\n",
              "      <td>71</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.053030</td>\n",
              "      <td>0.174242</td>\n",
              "      <td>0.037879</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.106523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>13628</td>\n",
              "      <td>The mood was created by the author in the memo...</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>4.033333</td>\n",
              "      <td>21</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.852</td>\n",
              "      <td>0.061781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>13629</td>\n",
              "      <td>In the memoir \"Narciso Rodriguez\", the mood cr...</td>\n",
              "      <td>4</td>\n",
              "      <td>166</td>\n",
              "      <td>9</td>\n",
              "      <td>4.421687</td>\n",
              "      <td>93</td>\n",
              "      <td>0.066265</td>\n",
              "      <td>0.246988</td>\n",
              "      <td>0.114458</td>\n",
              "      <td>0.222892</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.103630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>13630</td>\n",
              "      <td>The mood created @CAPS3 the author, Narciso Ro...</td>\n",
              "      <td>3</td>\n",
              "      <td>132</td>\n",
              "      <td>6</td>\n",
              "      <td>4.492424</td>\n",
              "      <td>86</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.037879</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.805</td>\n",
              "      <td>0.165012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>13631</td>\n",
              "      <td>The author created such a specific mood for th...</td>\n",
              "      <td>2</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>4.326316</td>\n",
              "      <td>62</td>\n",
              "      <td>0.073684</td>\n",
              "      <td>0.231579</td>\n",
              "      <td>0.042105</td>\n",
              "      <td>0.168421</td>\n",
              "      <td>0.073684</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.091583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1805 rows × 16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-983accbe-6974-4759-89f9-314a7882e225')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-983accbe-6974-4759-89f9-314a7882e225 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-983accbe-6974-4759-89f9-314a7882e225');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "features_set1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aAkFSMF_dvUT"
      },
      "outputs": [],
      "source": [
        "features_set1.to_csv('features.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}